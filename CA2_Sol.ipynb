{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "# execute this cell before you start\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CA2\n",
    "## due on 8/03/2019\n",
    "\n",
    "to submit the assignment, please do the following:\n",
    "\n",
    "- do `Cell -> All output -> Clear` to clear all your output\n",
    "- save the notebook (CA3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reuters newswire data\n",
    "\n",
    "Consider the data in  `keras.datasets.reuters` and train a network which reliably categorizes the newswires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, restricting the number of words to the most frequent 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "254e3a14f487f838d7fb74ad10a79f90",
     "grade": true,
     "grade_id": "cell-62578aed3a4137c9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "num_words=10000\n",
    "reuters = keras.datasets.reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=num_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_classes` is the number of unique labels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 8982\n",
      "2246 2246\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(train_labels))\n",
    "print(len(test_data), len(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the word index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index 0 is unused. Use it to set the reserved `<PAD>` string used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_index` maps words to the word code. Create a reverse map from word code to word and a `decode_article` function to view articles in human readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {}\n",
    "for key, value in word_index.items():\n",
    "    reverse_word_index[value] = key\n",
    "\n",
    "def decode_article(article):\n",
    "    decodedArticle = \"\"\n",
    "    for word_code in article:\n",
    "        decodedArticle += \" \"\n",
    "        decodedArticle += reverse_word_index.get(word_code, \"?\")\n",
    "    return decodedArticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have uniform input data, we make sure every article is a uniform 256 word long and pad the data with `<PAD>` when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that padding and trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the of of mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_article(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the model, compile it and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neeraj/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "8982/8982 [==============================] - 1s 120us/step - loss: 2.4391 - acc: 0.3519\n",
      "Epoch 2/40\n",
      "8982/8982 [==============================] - 1s 94us/step - loss: 1.8655 - acc: 0.4853\n",
      "Epoch 3/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 1.6166 - acc: 0.6019\n",
      "Epoch 4/40\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 1.4660 - acc: 0.6423\n",
      "Epoch 5/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 1.3272 - acc: 0.6752\n",
      "Epoch 6/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 1.1693 - acc: 0.7131\n",
      "Epoch 7/40\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 1.0304 - acc: 0.7403\n",
      "Epoch 8/40\n",
      "8982/8982 [==============================] - 1s 92us/step - loss: 0.9238 - acc: 0.7675\n",
      "Epoch 9/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 0.8341 - acc: 0.7901\n",
      "Epoch 10/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 0.7578 - acc: 0.8105\n",
      "Epoch 11/40\n",
      "8982/8982 [==============================] - 1s 93us/step - loss: 0.6893 - acc: 0.8249\n",
      "Epoch 12/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.6288 - acc: 0.8378\n",
      "Epoch 13/40\n",
      "8982/8982 [==============================] - 1s 118us/step - loss: 0.5737 - acc: 0.8507\n",
      "Epoch 14/40\n",
      "8982/8982 [==============================] - 1s 107us/step - loss: 0.5249 - acc: 0.8647\n",
      "Epoch 15/40\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.4841 - acc: 0.8772\n",
      "Epoch 16/40\n",
      "8982/8982 [==============================] - 1s 95us/step - loss: 0.4418 - acc: 0.8918\n",
      "Epoch 17/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.4089 - acc: 0.8991\n",
      "Epoch 18/40\n",
      "8982/8982 [==============================] - 1s 95us/step - loss: 0.3789 - acc: 0.9034\n",
      "Epoch 19/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.3507 - acc: 0.9108\n",
      "Epoch 20/40\n",
      "8982/8982 [==============================] - 1s 94us/step - loss: 0.3253 - acc: 0.9177\n",
      "Epoch 21/40\n",
      "8982/8982 [==============================] - 1s 95us/step - loss: 0.3016 - acc: 0.9213\n",
      "Epoch 22/40\n",
      "8982/8982 [==============================] - 1s 95us/step - loss: 0.2841 - acc: 0.9255\n",
      "Epoch 23/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.2627 - acc: 0.9287\n",
      "Epoch 24/40\n",
      "8982/8982 [==============================] - 1s 94us/step - loss: 0.2492 - acc: 0.9320\n",
      "Epoch 25/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.2366 - acc: 0.9334\n",
      "Epoch 26/40\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.2213 - acc: 0.9404\n",
      "Epoch 27/40\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.2111 - acc: 0.9432\n",
      "Epoch 28/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.1990 - acc: 0.9421\n",
      "Epoch 29/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.1883 - acc: 0.9451\n",
      "Epoch 30/40\n",
      "8982/8982 [==============================] - 1s 101us/step - loss: 0.1831 - acc: 0.9454\n",
      "Epoch 31/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.1775 - acc: 0.9458\n",
      "Epoch 32/40\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.1690 - acc: 0.9479\n",
      "Epoch 33/40\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.1631 - acc: 0.9491\n",
      "Epoch 34/40\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.1577 - acc: 0.9487\n",
      "Epoch 35/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.1544 - acc: 0.9508\n",
      "Epoch 36/40\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 0.1485 - acc: 0.9520\n",
      "Epoch 37/40\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 0.1448 - acc: 0.9519\n",
      "Epoch 38/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.1420 - acc: 0.9521\n",
      "Epoch 39/40\n",
      "8982/8982 [==============================] - 1s 101us/step - loss: 0.1413 - acc: 0.9525\n",
      "Epoch 40/40\n",
      "8982/8982 [==============================] - 1s 110us/step - loss: 0.1360 - acc: 0.9530\n",
      "2246/2246 [==============================] - 0s 67us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.738140771989831, 0.74933214603739984)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = num_words\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=train_data, y=train_labels, epochs=40)\n",
    "test_loss, test_acc = model.evaluate(x=test_data, y=test_labels)\n",
    "test_loss, test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
