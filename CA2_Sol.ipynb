{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "# execute this cell before you start\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CA2\n",
    "## due on 8/03/2019\n",
    "\n",
    "to submit the assignment, please do the following:\n",
    "\n",
    "- do `Cell -> All output -> Clear` to clear all your output\n",
    "- save the notebook (CA3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reuters newswire data\n",
    "\n",
    "Consider the data in  `keras.datasets.reuters` and train a network which reliably categorizes the newswires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "254e3a14f487f838d7fb74ad10a79f90",
     "grade": true,
     "grade_id": "cell-62578aed3a4137c9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "reuters = keras.datasets.reuters\n",
    "num_words=1000\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=num_words)\n",
    "\n",
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 2246)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "word_index[\"<PAD>\"] = 0\n",
    "\n",
    "\n",
    "reverse_word_index = {}\n",
    "for key, value in word_index.items() :\n",
    "    reverse_word_index[value] = key\n",
    "\n",
    "#reverse_word_index\n",
    "\n",
    "def decode_article(article):\n",
    "    decodedArticle = \"\"\n",
    "    for word_code in article:\n",
    "        decodedArticle += \" \"\n",
    "        decodedArticle += reverse_word_index.get(word_code, \"?\")\n",
    "    return decodedArticle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the of of mln loss for plc said at only ended said of could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 of of several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed of april 0 are 2 states will billion total and against 000 pct dlrs <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")\n",
    "\n",
    "decode_article(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000)\\ntrain_data = tokenizer.sequences_to_matrix(train_data, mode='binary')\\ntest_data = tokenizer.sequences_to_matrix(test_data, mode='binary')\\n\\ntrain_labels = keras.utils.to_categorical(train_labels, num_classes)\\ntest_labels = keras.utils.to_categorical(test_labels, num_classes)\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "train_data = tokenizer.sequences_to_matrix(train_data, mode='binary')\n",
    "test_data = tokenizer.sequences_to_matrix(test_data, mode='binary')\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neeraj/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "8982/8982 [==============================] - 1s 164us/step - loss: 2.4592 - acc: 0.3560\n",
      "Epoch 2/40\n",
      "8982/8982 [==============================] - 1s 74us/step - loss: 1.9235 - acc: 0.4734\n",
      "Epoch 3/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 1.6684 - acc: 0.5835\n",
      "Epoch 4/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 1.5481 - acc: 0.6123\n",
      "Epoch 5/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 1.4592 - acc: 0.6350\n",
      "Epoch 6/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 1.3564 - acc: 0.6668\n",
      "Epoch 7/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 1.2517 - acc: 0.6887\n",
      "Epoch 8/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 1.1758 - acc: 0.7083\n",
      "Epoch 9/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 1.1185 - acc: 0.7184\n",
      "Epoch 10/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 1.0737 - acc: 0.7305\n",
      "Epoch 11/40\n",
      "8982/8982 [==============================] - 1s 83us/step - loss: 1.0313 - acc: 0.7411\n",
      "Epoch 12/40\n",
      "8982/8982 [==============================] - 1s 74us/step - loss: 0.9948 - acc: 0.7508\n",
      "Epoch 13/40\n",
      "8982/8982 [==============================] - 1s 67us/step - loss: 0.9608 - acc: 0.7589\n",
      "Epoch 14/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.9311 - acc: 0.7705\n",
      "Epoch 15/40\n",
      "8982/8982 [==============================] - 1s 68us/step - loss: 0.9017 - acc: 0.7767\n",
      "Epoch 16/40\n",
      "8982/8982 [==============================] - 1s 68us/step - loss: 0.8762 - acc: 0.7829\n",
      "Epoch 17/40\n",
      "8982/8982 [==============================] - 1s 69us/step - loss: 0.8533 - acc: 0.7900\n",
      "Epoch 18/40\n",
      "8982/8982 [==============================] - 1s 69us/step - loss: 0.8325 - acc: 0.7947\n",
      "Epoch 19/40\n",
      "8982/8982 [==============================] - 1s 69us/step - loss: 0.8092 - acc: 0.7982\n",
      "Epoch 20/40\n",
      "8982/8982 [==============================] - 1s 80us/step - loss: 0.7919 - acc: 0.8010\n",
      "Epoch 21/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.7719 - acc: 0.8049\n",
      "Epoch 22/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.7611 - acc: 0.8054\n",
      "Epoch 23/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.7428 - acc: 0.8121\n",
      "Epoch 24/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.7255 - acc: 0.8185\n",
      "Epoch 25/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.7092 - acc: 0.8181\n",
      "Epoch 26/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.6952 - acc: 0.8240\n",
      "Epoch 27/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.6799 - acc: 0.8281\n",
      "Epoch 28/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.6667 - acc: 0.8312\n",
      "Epoch 29/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.6546 - acc: 0.8339\n",
      "Epoch 30/40\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 0.6395 - acc: 0.8359\n",
      "Epoch 31/40\n",
      "8982/8982 [==============================] - 1s 86us/step - loss: 0.6291 - acc: 0.8402\n",
      "Epoch 32/40\n",
      "8982/8982 [==============================] - 1s 79us/step - loss: 0.6163 - acc: 0.8448\n",
      "Epoch 33/40\n",
      "8982/8982 [==============================] - 1s 84us/step - loss: 0.6042 - acc: 0.8462\n",
      "Epoch 34/40\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.5923 - acc: 0.8508\n",
      "Epoch 35/40\n",
      "8982/8982 [==============================] - 1s 83us/step - loss: 0.5839 - acc: 0.8505\n",
      "Epoch 36/40\n",
      "8982/8982 [==============================] - 1s 83us/step - loss: 0.5716 - acc: 0.8540\n",
      "Epoch 37/40\n",
      "8982/8982 [==============================] - 1s 72us/step - loss: 0.5601 - acc: 0.8546\n",
      "Epoch 38/40\n",
      "8982/8982 [==============================] - 1s 78us/step - loss: 0.5516 - acc: 0.8619\n",
      "Epoch 39/40\n",
      "8982/8982 [==============================] - 1s 78us/step - loss: 0.5426 - acc: 0.8619\n",
      "Epoch 40/40\n",
      "8982/8982 [==============================] - 1s 71us/step - loss: 0.5337 - acc: 0.8621\n",
      "2246/2246 [==============================] - 0s 143us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1416884018498028, 0.75289403383793407)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_size = num_words\n",
    "#vocab_size = 10000\n",
    "#output_layer_size = len(set(train_labels))\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=train_data, y=train_labels, epochs=40)\n",
    "test_loss, test_acc = model.evaluate(x=test_data, y=test_labels)\n",
    "test_loss, test_acc\n",
    "\n",
    "# the final layer needs to be the size of train_labels ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index[min(list(reverse_word_index.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23568"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reverse_word_index[100]\n",
    "#train_data[1]\n",
    "#decode_article(train_data[1])\n",
    "max(reverse_word_index, key=lambda k:reverse_word_index[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zwermann'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index[23568]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
