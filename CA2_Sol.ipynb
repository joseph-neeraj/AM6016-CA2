{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "# execute this cell before you start\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CA2\n",
    "## due on 8/03/2019\n",
    "\n",
    "to submit the assignment, please do the following:\n",
    "\n",
    "- do `Cell -> All output -> Clear` to clear all your output\n",
    "- save the notebook (CA3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reuters newswire data\n",
    "\n",
    "Consider the data in  `keras.datasets.reuters` and train a network which reliably categorizes the newswires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, restricting the number of words to the most frequent 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "254e3a14f487f838d7fb74ad10a79f90",
     "grade": true,
     "grade_id": "cell-62578aed3a4137c9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "num_words=1000\n",
    "reuters = keras.datasets.reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=num_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_classes` is the number of unique labels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 2246)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index 0 is unused. Use it to set the reserved `<PAD>` used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index maps words to the word code. So create a reverse map from word code to word and a `decode_article` function so that it allows us to view articles in human readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {}\n",
    "for key, value in word_index.items():\n",
    "    reverse_word_index[value] = key\n",
    "\n",
    "def decode_article(article):\n",
    "    decodedArticle = \"\"\n",
    "    for word_code in article:\n",
    "        decodedArticle += \" \"\n",
    "        decodedArticle += reverse_word_index.get(word_code, \"?\")\n",
    "    return decodedArticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have uniform input data, we make sure every article is a uniform 256 word long and pad the data with `<PAD>` when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the of of mln loss for plc said at only ended said of could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 of of several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed of april 0 are 2 states will billion total and against 000 pct dlrs <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], maxlen=256, padding=\"post\")\n",
    "\n",
    "decode_article(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the model, compile it and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neeraj/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "8982/8982 [==============================] - 2s 168us/step - loss: 2.4231 - acc: 0.3634\n",
      "Epoch 2/40\n",
      "8982/8982 [==============================] - 0s 53us/step - loss: 1.8901 - acc: 0.4876\n",
      "Epoch 3/40\n",
      "8982/8982 [==============================] - 0s 53us/step - loss: 1.6622 - acc: 0.5819\n",
      "Epoch 4/40\n",
      "8982/8982 [==============================] - 1s 62us/step - loss: 1.5390 - acc: 0.6146\n",
      "Epoch 5/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 1.4464 - acc: 0.6378\n",
      "Epoch 6/40\n",
      "8982/8982 [==============================] - 1s 62us/step - loss: 1.3472 - acc: 0.6659\n",
      "Epoch 7/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 1.2461 - acc: 0.6903\n",
      "Epoch 8/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 1.1596 - acc: 0.7129\n",
      "Epoch 9/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 1.0896 - acc: 0.7298\n",
      "Epoch 10/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 1.0373 - acc: 0.7419\n",
      "Epoch 11/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.9971 - acc: 0.7535\n",
      "Epoch 12/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.9627 - acc: 0.7602\n",
      "Epoch 13/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 0.9312 - acc: 0.7700\n",
      "Epoch 14/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.9001 - acc: 0.7764\n",
      "Epoch 15/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.8754 - acc: 0.7806\n",
      "Epoch 16/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.8482 - acc: 0.7912\n",
      "Epoch 17/40\n",
      "8982/8982 [==============================] - 1s 69us/step - loss: 0.8257 - acc: 0.7951\n",
      "Epoch 18/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.8053 - acc: 0.7998\n",
      "Epoch 19/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.7832 - acc: 0.8037\n",
      "Epoch 20/40\n",
      "8982/8982 [==============================] - 1s 72us/step - loss: 0.7686 - acc: 0.8061\n",
      "Epoch 21/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.7499 - acc: 0.8127\n",
      "Epoch 22/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.7338 - acc: 0.8153\n",
      "Epoch 23/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.7170 - acc: 0.8205\n",
      "Epoch 24/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.7032 - acc: 0.8226\n",
      "Epoch 25/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.6900 - acc: 0.8261\n",
      "Epoch 26/40\n",
      "8982/8982 [==============================] - 1s 73us/step - loss: 0.6771 - acc: 0.8285\n",
      "Epoch 27/40\n",
      "8982/8982 [==============================] - 1s 67us/step - loss: 0.6622 - acc: 0.8322\n",
      "Epoch 28/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.6502 - acc: 0.8338\n",
      "Epoch 29/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.6384 - acc: 0.8400\n",
      "Epoch 30/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.6288 - acc: 0.8426\n",
      "Epoch 31/40\n",
      "8982/8982 [==============================] - 1s 66us/step - loss: 0.6166 - acc: 0.8426\n",
      "Epoch 32/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.6069 - acc: 0.8467\n",
      "Epoch 33/40\n",
      "8982/8982 [==============================] - 1s 74us/step - loss: 0.5956 - acc: 0.8490\n",
      "Epoch 34/40\n",
      "8982/8982 [==============================] - 1s 72us/step - loss: 0.5877 - acc: 0.8491\n",
      "Epoch 35/40\n",
      "8982/8982 [==============================] - 1s 87us/step - loss: 0.5765 - acc: 0.8536\n",
      "Epoch 36/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.5692 - acc: 0.8569\n",
      "Epoch 37/40\n",
      "8982/8982 [==============================] - 1s 64us/step - loss: 0.5591 - acc: 0.8586\n",
      "Epoch 38/40\n",
      "8982/8982 [==============================] - 1s 63us/step - loss: 0.5522 - acc: 0.8616\n",
      "Epoch 39/40\n",
      "8982/8982 [==============================] - 1s 62us/step - loss: 0.5443 - acc: 0.8602\n",
      "Epoch 40/40\n",
      "8982/8982 [==============================] - 1s 65us/step - loss: 0.5325 - acc: 0.8621\n",
      "2246/2246 [==============================] - 0s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1568343996046275, 0.74621549421193234)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = num_words\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=train_data, y=train_labels, epochs=40)\n",
    "test_loss, test_acc = model.evaluate(x=test_data, y=test_labels)\n",
    "test_loss, test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
